---
title: Lecture 03 GFS
author: "Soul Mate"
categories: ["MIT-6.824"]
date: 2019-06-20
url: "/distributed-systems/mit-6.824/lec03/gfs-translate.html"
---

[原文](/distributed-systems/mit-6.824/lec03/gfs.html)

## 为什么我们要阅读这篇论文
- GFS文件系统用于 map/reduce
- 这篇论文展示了6.824的主题，简单的一致性和性能

后续设计动机

- 好的系统论文 - 从应用程序到网络的所有细节

性能、容错、一致性

- 影响

大多数系统使用GFS（例如：Bigtable、Spanner @Google）

- HDFS (Hadoop Distributed File System) based on GFS

## 什么是一致性？

- 正确性条件

- 数据复制很重要，但实现起来很困难

  - 特别是当应用程序同时访问它时

  - [示意: 简单示例, 单台机器]

  - 如果应用程序写入，后续读的机器观察到什么？
    - 如果读取到的数据来自不同的机器怎么办？
  - 但是对于复制，每次写操作也必须在其他机器上 

- [示意：两台以上机器，读和写交叉]

  很明显我们的问题在这里。

- 弱一致性

  `read()` 可能返回陈旧数据 --- 不是最近写入的数据

- 强一致性

  `read()` 总是返回 `write()`最近写入的数据

- 两者之间的关系

  对于程序编写人员来说，强一致性更容易实现

  强一致性的性能比较差

  弱一致性具有良好的性能，容易扩展到多台服务器

  弱一致性很难理解

- 许多权衡取决于不同的正确性条件

  这里称他们为 “一致性模型”

  今天先看一眼；几乎每篇论文中都会出现他们。

## 理想的一致性模型

- 让我们回到单机的情况

- 如果复制的FS（File System）表现的像一个非复制的文件系统，那会很好吗？

  [示意：同一台机器上的许多客户机访问单个磁盘上的文件]

- 如果一个应用程序写，后续的读操作将会观察到写操作

- 如果两个应用程序并发地写入同一个文件会怎么样

  Q：在一台机器上会发生什么？

  在文件系统中经常是未定义的 --- 文件可能有一些混合内容

- 如果两个应用程序同时写入同一目录会怎么样？

  Q：在单台机器上会发生什么？

  一个先写入，另一个在写入（使用锁）

## 实现理想的一致性模型的挑战

- 并发 -- 正如我们看到的；事实上有很多的磁盘

- 机器故障 -- 任何操作都可能无法完成

- 网络分区 -- 可能无法到达每台机器/磁盘

- 为什么这些挑战很难克服？
  - 客户端和服务端之间需要通信
    - 性能
  - 协议可能变的复杂 --- 参见下周
    - 系统难以正确实现
  - 6.824许多系统并不提供理想的功能
    - GFS就是一个例子

## GFS目标：

- 有如此多的机器，故障是常见的
  - 必须容错
  - 假设机器每年故障一次，如果有1000台机器，每天大约有3台会出现故障
- 高性能的：许多并发的读和写
  - Map/Reduct 任务读取并将最终结果存储在GFS中
  - Note:  不是临时文件，是中间文件
- 有效利用网络：节省带宽
- 这些困难与理想的一致性相结合



## 高级设计 / 读

 [Figure 1 diagram, master + chunkservers]

- Master 存储目录，文件，名称，打开/读/写

  - 但不是POSIX

- 有100个Linux磁盘块服务器

  - 存储64MB的块（每个块对应普通的Linux文件）

  - 每个块在三个服务器上复制

  - Q：除了数据可用性，3X复制还提供了什么？

    读取热文件的负载均衡

    相近

  - Q：为什么不将每个文件的一个副本存储在RAID磁盘上呢

    RAID不是商品

    需要整机容错，而非存储设备

  - 为什么Chunk如此大？

    分摊压力，减少Master的状态大小

  - GFS master 服务器需要知道目录层级结构

    - 对于目录，其中包含文件

    - 对于文件，知道每个64MB的块服务器

  - Master将状态保持在内存中
    - 每个块64个字节元数据
  - Master具有元数据的私有可恢复数据库
    - 操作日志刷到磁盘
    - 偶尔的异步压缩checkpoint信息
    - N.B.: !=  Â§2.7.2 的应用程序检查点
  - Master能从故障中快速恢复
    - 影子Master落后Master
      - 能够提升为Master吗（不能）

- 客户端读

  - 发送文件名和chunk索引到master
  - master使用一组具有该chunk的服务器进行应答
    - 响应包含版本 # chunk
    - 客户端缓存这些信息
  - 询问附近的Chunk服务器
    - 检查版本 #
    - 如果版本号 #错误，重新联系master

## 写

 [Figure 2-style diagram with file offset sequence]

- 随机的客户端写入已有文件
  - 客户端向master询问chunk位置 + 主chunk服务器
  - master响应给客户端chunk服务器，版本 #, 谁是主chunk
    - 主chunk服务器有 (或得到) 60s租约
  - 客户端基于网络拓扑计算副本链
  - 客户端发送数据到第一个副本，该副本在转个其他副本
    - 网络使用的管道，分布负载
  - 副本回复数据接收
  - 客户端告诉主chunk写入
    - 主chunk分配序列号并写入
    - 然后告诉其他副本去写入
    - 所有写入完毕，回复客户端
  - 如果有另外的并发客户端像相同的位置写入数据该怎么办？
    - 客户端2在客户端1之后，覆盖数据
    - 现在客户端2再次写入，这次得到优先顺序（客户端1可能慢了）
    - 客户端2写入，但客户端1会覆盖
    - => 所有的副本都具有相同的数据（=一致），但混合来自C1/C2的部分（=未定义）
  - 客户端追加（不是记录追加）
    - 同样的处理，但C1和C2可以按任意的顺序排序
    - 一致，但未定义
    - 或者，如果仅有一个客户端写入，没有问题 --- 一致并且是定义的 

## 记录追加

- 客户端记录追加
  - 客户端像master请求chunk位置
  - 客户端推送数据到副本，但没有指定偏移量
  - 当数据推送到所有的chunk服务器上时，客户端联系主chunk服务器
    - 主chunk服务器分配序列号
    - 主chunk服务器检查追加的chunk是否合适
      - 如果不合适，填充chunk你 
    - 主chunk服务器选择追加的偏移量
    - 主chunk服务器应用本地更改
    - 主chunk服务器将所有请求转发给副本
    - 让我们看看R3在应用写操作的中途失败
    - 主chunk服务器检查到错误，告诉客户端重试
  - 客户端联系master后重试
    - master在此期间也许提出了R4（或者R3回来了）
    - 一个副本在字节序列中有一个间隙，所以不能只是追加
    - 填充所有副本的下一个可用偏移量
    - 主chunk服务器和二级chunk服务器应用写入
    - 主chunk服务器收到所有副本响应后，响应给客户端

## 内部管理

- 如果master不刷新租约，master可以指定一个新的主chunk服务器
- 如果副本数量低于某个数字，则master会复制chunk
- master平衡副本

## 故障

- chunk服务器容易被替换
  - 失败可能会导致一些客户端重试 （& duplicate records）
- Master:  宕机 -> GFS不可用
  - 影子master可以提供只读操作，可能会返回陈旧的数据
  - Q：为什么影子master不提供写操作
    - 脑裂（参见下一课程）

## GFS是否达到了理想的一致性

- 两个案例：目录和文件

- 目录：是，但是...

  - 是：强一致性（只有一份）
  - 但是：master并不总是可用的，伸缩性限制

- 文件：不一定

  - 有原子追加变异
    - 记录可以在两个偏移量处复制

  - 没有原子追加变异
    - 几个客户端的数据可能混合在一起
    - 如果你在意，使用原子追加或临时文件并且原子的重命名
  - 不幸的客户端可以在短时间内读取陈旧数据
    - 失败的追加会导致chunk不一致
      - 主chunk服务器更新不一致的chunk
      - 但是后来失败了，复制品也过时了
  - 客户端可能读取不是最新的块
  - 当客户端刷新租约时，它将了解新版本

## 作者声称弱一致性对于应用不是一个大问题

- 大多数文件都是追加更新
  - 应用程序可以在追加记录中使用UID来检测重复项
  - 应用程序可能只读取更改少的数据（而不是陈旧的数据）
- 应用程序可以使用临时文件和原子重命名

## 性能

- 巨大的读取总吞吐量（3个副本，分段）
  - 总计125MB/s
  - 接近饱和的网络
- 不同文件的写操作可能低于最大值
  - 作者将此问题归咎于他们的网络堆栈
  - 这会导致该副本的延迟传播到下一个副本
- 并发追加到单个文件
  - 受存储chunk最后一个限制
- 15年来，数字和细节发生了很大变化

## 摘要

- 性能、容错、一致性案例学习
  - 专用于MapReduce应用
- 什么在GFS中工作的好？
  - 大量的连续读和写
  - 追加
  - 巨大的吞吐量（3个拷贝，分段）
  - 数据容错（3个副本）
- 什么在GFS中工作的不好？
  - master容错
  - 小文件（master瓶颈）
  - 客户端可能看到陈旧的数据
  - 追加或许是重复的

## 参考

  http://queue.acm.org/detail.cfm?id=1594206  (discussion of gfs evolution)
  http://highscalability.com/blog/2010/9/11/googles-colossus-makes-search-real-time-by-dumping-mapreduce.html